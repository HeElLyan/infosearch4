# -*- coding: utf-8 -*-
"""4задание.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a7zju4EQx0UjdFJifpkjbHziWiKPjhWO
"""

from google.colab import drive
drive.mount('gdrive')

input_path = 'gdrive/My Drive/4курс/Инфопоиск/result/text_outputs/'
input_path_tf_idf = 'gdrive/My Drive/4курс/Инфопоиск/result/tf_idf/'
input_tokens_path = 'gdrive/My Drive/4курс/Инфопоиск/result/tokens.txt'
input_lemmas_path = 'gdrive/My Drive/4курс/Инфопоиск/result/lemmas.txt'
default_output_path = 'gdrive/My Drive/4курс/Инфопоиск/result/'

import string
import codecs
from enum import unique
from itertools import chain
import re
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
sw_nltk = stopwords.words('english')

def get_words_from_file(input):

    # with open(input) as source, open(output, 'w') as destination:
    with codecs.open(input) as source:
      
        #get the text from file
        text = source.read()
        #lower the text
        text = str(text.lower())

        text = re.sub(r'\w*\d\w*', '', text)

        symb = '.,!;()[]-:"/\|$@^''&*%?'
        for char in symb:
            text = text.replace(char, "")

        #split words in text
        words = text.split()
    
    for word in words: 
        if word == '':
            words.remove(word)

    #reduce stopwords
    words = [word for word in words if word not in sw_nltk]      

    return words

N = 142

# get all texts from site
all_texts = []

for i in range(N):

    input = input_path + 'выкачка' + str(i + 1) + '.txt'

    with open(input) as source:
        words = get_words_from_file(input)

    all_texts.append(words)

len(all_texts)

print(len(all_texts[0]))

# get tokens
with open(input_tokens_path) as source:
    tokens = source.read()

tokens = tokens.split('\n')
del tokens[-1]

len(tokens)

"""**tf(t,d) = count of t in d / number of words in d**"""

# 3d list token entrance number in all texts
# [word from text, count of this word token in text]
tf_token = []

# token entrance number
for words in all_texts:

    # 2d list token entrance number in one text
    local_token_entrance_count = []

    for token in tokens:

        count = 0
        for word in words:
          
            if word == token:
                count += 1

        local_token_entrance_count.append([token, count])

    tf_token.append(local_token_entrance_count)

# tf_token[0]

# all_texts[0]

len(tf_token)

len(tf_token[0]) == len(tokens)

# test_count = 0
# for i in range(len(tf_token[0])):
#     test_count += int(tf_token[0][i][1])

# print(test_count)

# get lemmas
with open(input_lemmas_path) as source:
    lemmas = source.read()

lemmas = lemmas.split('\n')
del lemmas[-1]

len(lemmas)

keys = [lemmas[i].rstrip().split(":")[0] for i in range(len(lemmas))]
print(keys)

vals = []
for i in range(len(lemmas)):
    local_list = []
  
    default_lemma = lemmas[i].rstrip().replace(":", "").split(" ")

    for j in range(1, len(default_lemma)):
        local_res = default_lemma[j]
        local_list.append(local_res)

    vals.append(local_list)

print(vals)

len(keys)

# 3d list token entrance number in all texts
# [word from text, count of this word lemmas in text]
tf_lemma = []

# token entrance number
for words in all_texts:

    # 2d list token entrance number in one text
    local_lemma_entrance_count = []
    for i in range(len(vals)):

        for lemma in vals[i]:

            count = 0
            for word in words:

                if word == lemma:
                    count += 1

        local_lemma_entrance_count.append([keys[i], count])
        
    tf_lemma.append(local_lemma_entrance_count)

# tf_lemma[0][1000:]

print(len(tf_lemma))

len(tf_lemma[0]) == len(lemmas)

"""**df(t) = occurrence of t in N documents**"""

import numpy as np

df_tokens_files = []

for i in range(len(tokens)):
    count_loc = 0
    for j in range(len(tf_token)):
        count_loc += int(tf_token[j][i][1])

    df_tokens_files.append(count_loc)

# for i in range(len(df_tokens_files)):
#     if df_tokens_files[i] == 0:
#         print('yes')

len(df_tokens_files)

import numpy as np

df_lemmas_files = []

for i in range(len(lemmas)):
    count_loc = 0
    for j in range(len(tf_lemma)):
        count_loc += int(tf_lemma[j][i][1])

    df_lemmas_files.append(count_loc)

df_lemmas_files[10:13]

# for i in range(len(df_lemmas_files)):
#     if df_lemmas_files[i] == 0:
#         print('yes')

len(df_lemmas_files)

"""**idf(t) = log(N/(df + 1))**"""

N_tokens = len(tokens)

idf_tokens_files = [np.log(N_tokens / (df_tokens_files[i] + 1)) for i in range(len(df_tokens_files))]

idf_tokens_files[:3]

# for i in range(len(idf_tokens_files)):
#     if idf_tokens_files[i] == 0:
#         print('yes')

len(idf_tokens_files)

len(tf_token[0])

N_lemmas = len(lemmas)

idf_lemmas_files = [np.log(N_lemmas / (df_lemmas_files[i] + 1)) for i in range(len(df_lemmas_files))]

# for i in range(len(idf_lemmas_files)):
#     if idf_lemmas_files[i] == 0:
#         print('yes')

len(idf_lemmas_files)

idf_lemmas_files[:3]

len(tf_lemma[0])

tf_token[0][0]

# tf_token[0][1300:1350]

# idf_tokens_files[1300:1350]

"""**tf-idf(t, d) = tf(t, d) * log(N/(df + 1))**"""

tf_idf_tokens_files = []

for i in range(len(tf_token)):

    loc_list = []
    for j in range(len(tf_token[i])):
        loc_res = tf_token[i][j][1] * idf_tokens_files[j]
        loc_list.append([tf_token[i][j][0], loc_res])

    tf_idf_tokens_files.append(loc_list)

# tf_idf_tokens_files[0][1300:1350]

# tf_idf_tokens_files[0]

len(tf_idf_tokens_files)

len(tf_idf_tokens_files[0])

tf_idf_lemmas_files = []

for i in range(len(tf_lemma)):

    loc_list = []
    for j in range(len(tf_lemma[i])):
        loc_res = tf_lemma[i][j][1] * idf_lemmas_files[j]
        loc_list.append([tf_lemma[i][j][0], loc_res])

    tf_idf_lemmas_files.append(loc_list)

# tf_idf_lemmas_files[0]

len(tf_idf_lemmas_files[0])

idf_lemmas_files[0]

tf_idf_lemmas_files[0][1]

def write_data_to_file(output, data, idf, tf_idf):
    for i in range(N):
        with open(output + 'tf-idf' + str(i + 1) + '.txt', 'w') as destination:
            for j in range(len(data)):
                destination.write(data[j] + ' ' + str(idf[j]) + ' ' + str(tf_idf[i][j][1]) + '\n')

write_data_to_file(input_path_tf_idf + 'tokens-', tokens, idf_tokens_files, tf_idf_tokens_files)
write_data_to_file(input_path_tf_idf + 'lemmas-', lemmas, idf_lemmas_files, tf_idf_lemmas_files)